# pages = c(1:5)
# queries = c('business','sports','entertainment','economy','politics','science','health','art','technology','crime')
pages = c(1:2)
queries = c('business','sports')
news_data = data.frame()
data_count = list()
sum_data_count = list()
for(query in queries){
for (page in pages){
url = paste(api, 'q=', query, '&page-size=', page_size, '&page=', page, '&api-key=', api_key,
'&show-fields=body', sep = "")
json = fromJSON(url)
body = as.data.frame(json$response$results$fields)
data = as.data.frame(json$response$results)
data = subset(data, select = -c(fields))
data = cbind(data,body)
data_count = append(data_count,nrow(data))
news_data = rbind(news_data,data)
}
select_data=select(data,c("sectionName","body"))
print(kable(head(select_data), format = "latex", booktabs = T,
caption=paste("Table containing",toupper(query)," News")) %>%
kable_styling(latex_options = c("striped","hold_position","scale_down")))
cat("\n")
sum_data_count = append(sum_data_count,sum(as.integer(data_count)))
data_count = 0
}
library(tidyverse)
library(reshape)
library(textclean)
library(qdap)
print("Columns in the News dataframe before cleaning:")
names(news_data)
kable(head(news_data), format = "latex", booktabs = T,
caption=paste("Table containing Unclean News data")) %>%
kable_styling(latex_options = c("striped","hold_position","scale_down"))
news = news_data %>%
select (c("id", "pillarName", "sectionName", "webTitle", "body"))%>%
rename(
c(
'pillarName' = 'type',
'sectionName' = 'section',
'webTitle' = 'title'
)
)%>%
na.omit()
abv = c("in.", "ft.", "bn")
repl = c("inch", "feet", " billion")
clean_function = function(data){
data = str_replace_all(data,"<.*?>", " ")
data = tolower(data)
data = trimws(data)
data = lapply(data, function(x) gsub("[[:punct:]]", " ", x))
data = replace_curly_quote(data)
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_money(data)
data = replace_ordinal(data)
data = replace_number(data)
data = replace_symbol(data)
#data = replace_contraction(data)
data = replace_date(data)
data = replace_abbreviation(data)
data = replace_abbreviation(data, abv, repl)
data = str_squish(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = str_replace_all(data,"[^a-zA-Z\\s]", "")
}
news$body =  clean_function(news$body)
library(tidyverse)
library(reshape)
library(textclean)
library(qdap)
print("Columns in the News dataframe before cleaning:")
names(news_data)
kable(head(news_data), format = "latex", booktabs = T,
caption=paste("Table containing Unclean News data")) %>%
kable_styling(latex_options = c("striped","hold_position","scale_down"))
news = news_data %>%
select (c("id", "pillarName", "sectionName", "webTitle", "body"))%>%
rename(
c(
'pillarName' = 'type',
'sectionName' = 'section',
'webTitle' = 'title'
)
)%>%
na.omit()
abv = c("in.", "ft.", "bn")
repl = c("inch", "feet", " billion")
clean_function = function(data){
data = str_replace_all(data,"<.*?>", " ")
data = tolower(data)
data = trimws(data)
data = lapply(data, function(x) gsub("[[:punct:]]", " ", x))
data = replace_curly_quote(data)
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_money(data)
data = replace_ordinal(data)
data = replace_number(data)
data = replace_symbol(data)
#data = replace_contraction(data)
data = replace_date(data)
data = replace_abbreviation(data)
data = replace_abbreviation(data, abv, repl)
data = str_squish(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = str_replace_all(data,"[^a-zA-Z\\s]", "")
}
#news$body =  clean_function(news$body)
data = news$body
data = str_replace_all(data,"<.*?>", " ")
data = tolower(data)
data = trimws(data)
data = lapply(data, function(x) gsub("[[:punct:]]", " ", x))
data = replace_curly_quote(data)
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_money(data)
data = replace_ordinal(data)
data = replace_number(data)
library(jsonlite)
library(dplyr)
library(kableExtra)
api = 'https://content.guardianapis.com/search?'
api_key = '0e9a8c4e-1866-4503-a9eb-70e391c499c4'
page_size = '200'
# pages = c(1:5)
# queries = c('business','sports','entertainment','economy','politics','science','health','art','technology','crime')
pages = c(1:2)
queries = c('business','sports')
news_data = data.frame()
data_count = list()
sum_data_count = list()
for(query in queries){
for (page in pages){
url = paste(api, 'q=', query, '&page-size=', page_size, '&page=', page, '&api-key=', api_key,
'&show-fields=body', sep = "")
json = fromJSON(url)
body = as.data.frame(json$response$results$fields)
data = as.data.frame(json$response$results)
data = subset(data, select = -c(fields))
data = cbind(data,body)
data_count = append(data_count,nrow(data))
news_data = rbind(news_data,data)
}
select_data=select(data,c("sectionName","body"))
print(kable(head(select_data), format = "latex", booktabs = T,
caption=paste("Table containing",toupper(query)," News")) %>%
kable_styling(latex_options = c("striped","hold_position","scale_down")))
cat("\n")
sum_data_count = append(sum_data_count,sum(as.integer(data_count)))
data_count = 0
}
library(tidyverse)
library(reshape)
library(textclean)
library(qdap)
print("Columns in the News dataframe before cleaning:")
names(news_data)
kable(head(news_data), format = "latex", booktabs = T,
caption=paste("Table containing Unclean News data")) %>%
kable_styling(latex_options = c("striped","hold_position","scale_down"))
news = news_data %>%
select (c("id", "pillarName", "sectionName", "webTitle", "body"))%>%
rename(
c(
'pillarName' = 'type',
'sectionName' = 'section',
'webTitle' = 'title'
)
)%>%
na.omit()
abv = c("in.", "ft.", "bn")
repl = c("inch", "feet", " billion")
clean_function = function(data){
data = str_replace_all(data,"<.*?>", " ")
data = tolower(data)
data = trimws(data)
data = lapply(data, function(x) gsub("[[:punct:]]", " ", x))
data = replace_curly_quote(data)
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_money(data)
data = replace_ordinal(data)
data = replace_number(data)
data = replace_symbol(data)
data = replace_contraction(data)
data = replace_date(data)
data = replace_abbreviation(data)
data = replace_abbreviation(data, abv, repl)
data = str_squish(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = str_replace_all(data,"[^a-zA-Z\\s]", "")
}
news$body =  clean_function(news$body)
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(dplyr)
library(kableExtra)
api = 'https://content.guardianapis.com/search?'
api_key = '0e9a8c4e-1866-4503-a9eb-70e391c499c4'
page_size = '200'
# pages = c(1:5)
# queries = c('business','sports','entertainment','economy','politics','science','health','art','technology','crime')
pages = c(1:2)
queries = c('business','sports')
news_data = data.frame()
data_count = list()
sum_data_count = list()
for(query in queries){
for (page in pages){
url = paste(api, 'q=', query, '&page-size=', page_size, '&page=', page, '&api-key=', api_key,
'&show-fields=body', sep = "")
json = fromJSON(url)
body = as.data.frame(json$response$results$fields)
data = as.data.frame(json$response$results)
data = subset(data, select = -c(fields))
data = cbind(data,body)
data_count = append(data_count,nrow(data))
news_data = rbind(news_data,data)
}
select_data=select(data,c("sectionName","body"))
print(kable(head(select_data), format = "latex", booktabs = T,
caption=paste("Table containing",toupper(query)," News")) %>%
kable_styling(latex_options = c("striped","hold_position","scale_down")))
cat("\n")
sum_data_count = append(sum_data_count,sum(as.integer(data_count)))
data_count = 0
}
data = news$body
data = str_replace_all(data,"<.*?>", " ")
library(tidyverse)
library(reshape)
library(textclean)
library(qdap)
data = news$body
data = str_replace_all(data,"<.*?>", " ")
data = tolower(data)
data = trimws(data)
data = lapply(data, function(x) gsub("[[:punct:]]", " ", x))
data = replace_curly_quote(data)
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_money(data)
data = replace_ordinal(data)
data = replace_number(data)
library(tidyverse)
library(reshape)
library(textclean)
library(qdap)
data = news$body
data = str_replace_all(data,"<.*?>", " ")
data = tolower(data)
data = trimws(data)
data = lapply(data, function(x) gsub("[[:punct:]]", " ", x))
data = replace_curly_quote(data)
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_money(data)
data = replace_ordinal(data)
data[2] = replace_number(data[2])
data = replace_symbol(data)
data = replace_contraction(data)
data = replace_date(data)
data = replace_abbreviation(data)
data = replace_abbreviation(data, abv, repl)
data = str_squish(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = str_replace_all(data,"[^a-zA-Z\\s]", "")
data[2]
library(tidyverse)
library(reshape)
library(textclean)
library(qdap)
data = news$body
data = str_replace_all(data,"<.*?>", " ")
data = trimws(data)
data = lapply(data, function(x) gsub("[[:punct:]]", " ", x))
data = replace_curly_quote(data)
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_money(data)
data = replace_ordinal(data)
#data[2] = replace_number(data[2])
data = replace_symbol(data)
data = replace_contraction(data)
data = replace_date(data)
data = replace_abbreviation(data)
data = replace_abbreviation(data, abv, repl)
data = str_squish(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = str_replace_all(data,"[^a-zA-Z\\s]", "")
data = tolower(data)
data[2]
library(tidyverse)
library(reshape)
library(textclean)
library(qdap)
data = news$body
data = str_replace_all(data,"<.*?>", " ")
data = trimws(data)
data = lapply(data, function(x) gsub("[[:punct:]]", " ", x))
data = replace_curly_quote(data)
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_money(data)
data = replace_ordinal(data)
data = replace_number(data)
data = lapply(data, replace_number(data))
data = lapply(data, replace_number)
data = lapply(range(nrow(data)), replace_number(data[x]))
test = for rows in nrow(data){
test = for (rows in nrow(data)){
print(rows)
}
test = for (rows in nrow(data)){
print(rows)
}
test = for (rows in nrow(data)){
print(rows)
}
test
print(nrow(data))
test = for (rows in nrow(data)){
print(rows)
}
test
print(count(data))
data = news$body
(count(data))
length(data)
nrow(news)
length(data)
test = for (rows in length(data)){
print(rows)
}
test
test = for (rows in range(length(data))){
print(rows)
}
test
test = for (rows in 1:length(data)){
print(rows)
}
test
data = for (row in 1:length(data)){
data[row] = replace_number(data[row])
}
library(tidyverse)
library(reshape)
library(textclean)
library(qdap)
data = news$body
data = str_replace_all(data,"<.*?>", " ")
data = trimws(data)
data = lapply(data, function(x) gsub("[[:punct:]]", " ", x))
data = replace_curly_quote(data)
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_money(data)
data = replace_ordinal(data)
data = for (row in 1: 2){
data[row] = replace_number(data[row])
}
data[2]
data = replace_symbol(data)
data = replace_contraction(data)
data = replace_date(data)
data = replace_abbreviation(data)
data = replace_abbreviation(data, abv, repl)
data = str_squish(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = str_replace_all(data,"[^a-zA-Z\\s]", "")
data = tolower(data)
data[2]
for (row in 1: 2){
data[row] = replace_number(data[row])
}
data[2]
library(jsonlite)
library(dplyr)
library(kableExtra)
api = 'https://content.guardianapis.com/search?'
api_key = '0e9a8c4e-1866-4503-a9eb-70e391c499c4'
page_size = '200'
pages = c(1:5)
queries = c('business','sports','entertainment','economy','politics','science','crime','health','art','technology')
news_data = data.frame()
data_count = list()
sum_data_count = list()
for(query in queries){
for (page in pages){
url = paste(api, 'q=', query, '&page-size=', page_size, '&page=', page, '&api-key=', api_key,
sep = "")
json = fromJSON(url)
body = as.data.frame(json$response$results$fields$body)
data = as.data.frame(json$response$results)
data = select(json, -c(fields))
data = cbind(data,body)
data_count = append(data_count,nrow(data))
sum_data_count = append(sum_data_count,sum(as.integer(data_count)))
news_data = rbind(news_data,data)
}
# select_data=select(data,c("sectionName","webPublicationDate","webTitle"))
# print(kable(head(select_data), format = "latex", booktabs = T,
#         caption=paste("Table containing",toupper(query)," News")) %>%
#     kable_styling(latex_options = c("striped","hold_position")))
# cat("\n")
data_count = 0
}
library(jsonlite)
library(dplyr)
library(kableExtra)
api = 'https://content.guardianapis.com/search?'
api_key = '0e9a8c4e-1866-4503-a9eb-70e391c499c4'
page_size = '200'
# pages = c(1:5)
# queries = c('business','sports','entertainment','economy','politics','science','crime','health','art','technology')
pages = c(1:2)
queries = c('business','sports')
news_data = data.frame()
data_count = list()
sum_data_count = list()
for(query in queries){
for (page in pages){
url = paste(api, 'q=', query, '&page-size=', page_size, '&page=', page, '&api-key=', api_key,
sep = "")
json = fromJSON(url)
body = as.data.frame(json$response$results$fields$body)
data = as.data.frame(json$response$results)
data = select(json, -c(fields))
data = cbind(data,body)
data_count = append(data_count,nrow(data))
sum_data_count = append(sum_data_count,sum(as.integer(data_count)))
news_data = rbind(news_data,data)
}
# select_data=select(data,c("sectionName","webPublicationDate","webTitle"))
# print(kable(head(select_data), format = "latex", booktabs = T,
#         caption=paste("Table containing",toupper(query)," News")) %>%
#     kable_styling(latex_options = c("striped","hold_position")))
# cat("\n")
data_count = 0
}
library(jsonlite)
library(dplyr)
library(kableExtra)
api = 'https://content.guardianapis.com/search?'
api_key = '0e9a8c4e-1866-4503-a9eb-70e391c499c4'
page_size = '200'
# pages = c(1:5)
# queries = c('business','sports','entertainment','economy','politics','science','crime','health','art','technology')
pages = c(1)
queries = c('business')
news_data = data.frame()
data_count = list()
sum_data_count = list()
for(query in queries){
for (page in pages){
url = paste(api, 'q=', query, '&page-size=', page_size, '&page=', page, '&api-key=', api_key,
sep = "")
json = fromJSON(url)
body = as.data.frame(json$response$results$fields$body)
data = as.data.frame(json$response$results)
data = select(json, -c(fields))
data = cbind(data,body)
data_count = append(data_count,nrow(data))
sum_data_count = append(sum_data_count,sum(as.integer(data_count)))
news_data = rbind(news_data,data)
}
# select_data=select(data,c("sectionName","webPublicationDate","webTitle"))
# print(kable(head(select_data), format = "latex", booktabs = T,
#         caption=paste("Table containing",toupper(query)," News")) %>%
#     kable_styling(latex_options = c("striped","hold_position")))
# cat("\n")
data_count = 0
}
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(dplyr)
library(kableExtra)
api = 'https://content.guardianapis.com/search?'
api_key = '0e9a8c4e-1866-4503-a9eb-70e391c499c4'
page_size = '200'
# pages = c(1:5)
# queries = c('business','sports','entertainment','economy','politics','science','crime','health','art','technology')
pages = c(1)
queries = c('business')
news_data = data.frame()
data_count = list()
sum_data_count = list()
for(query in queries){
for (page in pages){
url = paste(api, 'q=', query, '&page-size=', page_size, '&page=', page, '&api-key=', api_key,
sep = "")
json = fromJSON(url)
body = as.data.frame(json$response$results$fields$body)
data = as.data.frame(json$response$results)
data = select(json, -c(fields))
data = cbind(data,body)
data_count = append(data_count,nrow(data))
sum_data_count = append(sum_data_count,sum(as.integer(data_count)))
news_data = rbind(news_data,data)
}
# select_data=select(data,c("sectionName","webPublicationDate","webTitle"))
# print(kable(head(select_data), format = "latex", booktabs = T,
#         caption=paste("Table containing",toupper(query)," News")) %>%
#     kable_styling(latex_options = c("striped","hold_position")))
# cat("\n")
data_count = 0
}
