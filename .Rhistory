pages = c(1:2)
queries = c('business','sports')
news_data = data.frame()
data_count = list()
sum_data_count = list()
for(query in queries){
for (page in pages){
url = paste(api, 'q=', query, '&page-size=', page_size, '&page=', page, '&show-fields=body', '&api-key=', api_key,
sep = "")
json = fromJSON(url)
body = as.data.frame(json$response$results$fields$body)
data = as.data.frame(json$response$results)
data = select(data, -c(fields))
# data = cbind(data,body)
# data_count = append(data_count,nrow(data))
# sum_data_count = append(sum_data_count,sum(as.integer(data_count)))
# news_data = rbind(news_data,data)
}
# select_data=select(data,c("sectionName","webPublicationDate","webTitle"))
# print(kable(head(select_data), format = "latex", booktabs = T,
#         caption=paste("Table containing",toupper(query)," News")) %>%
#     kable_styling(latex_options = c("striped","hold_position")))
# cat("\n")
data_count = 0
}
View(data)
View(news_data)
library(jsonlite)
library(dplyr)
library(kableExtra)
api = 'https://content.guardianapis.com/search?'
api_key = '0e9a8c4e-1866-4503-a9eb-70e391c499c4'
page_size = '200'
# pages = c(1:5)
# queries = c('business','sports','entertainment','economy','politics','science','crime','health','art','technology')
pages = c(1:2)
queries = c('business','sports')
news_data = data.frame()
data_count = list()
sum_data_count = list()
for(query in queries){
for (page in pages){
url = paste(api, 'q=', query, '&page-size=', page_size, '&page=', page, '&show-fields=body', '&api-key=', api_key,
sep = "")
json = fromJSON(url)
body = as.data.frame(json$response$results$fields$body)
data = as.data.frame(json$response$results)
data = select(data, -c(fields))
data = cbind(data,body)
data_count = append(data_count,nrow(data))
sum_data_count = append(sum_data_count,sum(as.integer(data_count)))
news_data = rbind(news_data,data)
}
# select_data=select(data,c("sectionName","webPublicationDate","webTitle"))
# print(kable(head(select_data), format = "latex", booktabs = T,
#         caption=paste("Table containing",toupper(query)," News")) %>%
#     kable_styling(latex_options = c("striped","hold_position")))
# cat("\n")
data_count = 0
}
View(news_data)
mapply(print(paste("Number of rows in ",query," News: ",data_count)),query = toupper(queries), data_count = sum_data_count)
print(queries)
print(data_count)
print(sum_data_count)
library(jsonlite)
library(dplyr)
library(kableExtra)
api = 'https://content.guardianapis.com/search?'
api_key = '0e9a8c4e-1866-4503-a9eb-70e391c499c4'
page_size = '200'
# pages = c(1:5)
# queries = c('business','sports','entertainment','economy','politics','science','crime','health','art','technology')
pages = c(1:2)
queries = c('business','sports')
news_data = data.frame()
data_count = list()
sum_data_count = list()
for(query in queries){
for (page in pages){
url = paste(api, 'q=', query, '&page-size=', page_size, '&page=', page, '&show-fields=body', '&api-key=', api_key,
sep = "")
json = fromJSON(url)
body = as.data.frame(json$response$results$fields$body)
data = as.data.frame(json$response$results)
data = select(data, -c(fields))
data = cbind(data,body)
data_count = append(data_count,nrow(data))
sum_data_count = append(sum_data_count,sum(as.integer(data_count)))
news_data = rbind(news_data,data)
}
# select_data=select(data,c("sectionName","webPublicationDate","webTitle"))
# print(kable(head(select_data), format = "latex", booktabs = T,
#         caption=paste("Table containing",toupper(query)," News")) %>%
#     kable_styling(latex_options = c("striped","hold_position")))
# cat("\n")
print(sum_data_count)
data_count = 0
}
View(news_data)
library(jsonlite)
library(dplyr)
library(kableExtra)
api = 'https://content.guardianapis.com/search?'
api_key = '0e9a8c4e-1866-4503-a9eb-70e391c499c4'
page_size = '200'
# pages = c(1:5)
# queries = c('business','sports','entertainment','economy','politics','science','crime','health','art','technology')
pages = c(1:2)
queries = c('business','sports')
news_data = data.frame()
data_count = list()
sum_data_count = list()
for(query in queries){
for (page in pages){
url = paste(api, 'q=', query, '&page-size=', page_size, '&page=', page, '&show-fields=body', '&api-key=', api_key,
sep = "")
json = fromJSON(url)
body = as.data.frame(json$response$results$fields)
data = as.data.frame(json$response$results)
data = subset(data, select = -c(fields))
data = cbind(data,body)
data_count = append(data_count,nrow(data))
news_data = rbind(news_data,data)
}
# select_data = select(data, c("sectionName","body"))
# print(kable(head(select_data), format = "latex", booktabs = T,
#         caption=paste("Table containing",toupper(query)," News")) %>%
#     kable_styling(latex_options = c("striped","hold_position")))
# cat("\n")
sum_data_count = append(sum_data_count,sum(as.integer(data_count)))
data_count = 0
}
View(news_data)
print_data = function(query,sum_data_count){
print(paste("Number of rows in ",toupper(query)," queries: ",sum_data_count))
cat("\n")
}
mapply(print_data,query = queries,sum_data_count = sum_data_count)
cat("\n")
print(paste("Total Number of rows in News: ",nrow(news_data)))
summary(news_data)
print("Columns in the News data before cleaning:")
names(news_data)
news = news_data%>%
select ( c("id", "pillarName", "sectionName", "webTitle", "body"))%>%
rename(
c(
'pillarName' = 'type',
'sectionName' = 'section',
'webTitle' = 'title'
)
)%>%
na.omit()
library(tidyverse)
library(reshape)
library(qdap)
library(textclean)
news = news_data%>%
select ( c("id", "pillarName", "sectionName", "webTitle", "body"))%>%
rename(
c(
'pillarName' = 'type',
'sectionName' = 'section',
'webTitle' = 'title'
)
)%>%
na.omit()
data = news$body[2]
data
data = str_replace_all(data, "<.*?>","")
data = lapply(data, function(x) gsub("[[:punct:]]"," ",x))
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_curly_quote(data)
data = replace_symbol(data)
data = replace_money(data)
data = replace_abbreviation(data)
data = replace_word_elongation(data)
data = replace_contraction(data)
data = replace_incomplete(data)
data = replace_date(data)
data = replace_ordinal(data)
#data = replace_number(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = tolower(data)
data = trimws(data)
data = str_squish(data)
data = str_replace_all(data, "[^a-zA-Z\\s]","")
data
data = news$body[2]
data
data = str_replace_all(data, "<.*?>","")
data = lapply(data, function(x) gsub("[[:punct:]]"," ",x))
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_curly_quote(data)
data = replace_symbol(data)
data = replace_money(data)
data = replace_abbreviation(data)
data = replace_word_elongation(data)
data = replace_contraction(data)
data = replace_incomplete(data)
data = replace_date(data)
data = replace_ordinal(data)
data = replace_number(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = tolower(data)
data = trimws(data)
data = str_squish(data)
data = str_replace_all(data, "[^a-zA-Z\\s]","")
data
data = news$body[2]
data
data = str_replace_all(data, "<.*?>","")
data = lapply(data, function(x) gsub("[[:punct:]]"," ",x))
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_curly_quote(data)
data = replace_symbol(data)
data = replace_money(data)
data = replace_abbreviation(data)
data = replace_word_elongation(data)
data = replace_contraction(data)
data = replace_incomplete(data)
data = replace_date(data)
data = replace_ordinal(data)
data = replace_number(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = tolower(data)
data = trimws(data)
data = str_squish(data)
data = str_replace_all(data, "[^a-zA-Z\\s]","")
data
data = news$body[2]
data = str_replace_all(data, "<.*?>","")
data = lapply(data, function(x) gsub("[[:punct:]]"," ",x))
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_curly_quote(data)
data = replace_symbol(data)
data = replace_money(data)
data = replace_abbreviation(data)
data = replace_word_elongation(data)
data = replace_contraction(data)
data = replace_incomplete(data)
data = replace_date(data)
data = replace_ordinal(data)
data = replace_number(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = tolower(data)
data = trimws(data)
data = str_squish(data)
data = str_replace_all(data, "[^a-zA-Z\\s]","")
data
library(tidyverse)
library(reshape)
library(qdap)
library(textclean)
print("Columns in the News data before cleaning:")
names(news_data)
kable(head(news_data), format = "latex", booktabs = T,
caption=paste("Table containing Unclean News Data")) %>%
kable_styling(latex_options = c("striped","hold_position"))
news = news_data%>%
select ( c("id", "pillarName", "sectionName", "webTitle", "body"))%>%
rename(
c(
'pillarName' = 'type',
'sectionName' = 'section',
'webTitle' = 'title'
)
)%>%
na.omit()
abv = c("bn")
rep = c("billion")
clean_function = function(data){
data = str_replace_all(data, "<.*?>","")
data = lapply(data, function(x) gsub("[[:punct:]]"," ",x))
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_curly_quote(data)
data = replace_symbol(data)
data = replace_money(data)
data = replace_abbreviation(data)
data = replace_word_elongation(data)
data = replace_contraction(data)
data = replace_incomplete(data)
data = replace_date(data)
data = replace_ordinal(data)
data = replace_number(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = tolower(data)
data = trimws(data)
data = str_squish(data)
data = str_replace_all(data, "[^a-zA-Z\\s]","")
}
news$body = clean_function(data)
print("Sample of Clean News Article Body:")
cat("\n")
news$body[2]
kable(head(news), format = "latex", booktabs = T,
caption=paste("Table containing Clean News Data")) %>%
kable_styling(latex_options = c("striped","hold_position"))
library(tidyverse)
library(reshape)
library(qdap)
library(textclean)
print("Columns in the News data before cleaning:")
names(news_data)
kable(head(news_data), format = "latex", booktabs = T,
caption=paste("Table containing Unclean News Data")) %>%
kable_styling(latex_options = c("striped","hold_position"))
news = news_data%>%
select ( c("id", "pillarName", "sectionName", "webTitle", "body"))%>%
rename(
c(
'pillarName' = 'type',
'sectionName' = 'section',
'webTitle' = 'title'
)
)%>%
na.omit()
abv = c("bn")
rep = c("billion")
clean_function = function(data){
data = str_replace_all(data, "<.*?>","")
data = lapply(data, function(x) gsub("[[:punct:]]"," ",x))
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_curly_quote(data)
data = replace_symbol(data)
data = replace_money(data)
data = replace_abbreviation(data)
data = replace_word_elongation(data)
data = replace_contraction(data)
data = replace_incomplete(data)
data = replace_date(data)
data = replace_ordinal(data)
data = replace_number(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = tolower(data)
data = trimws(data)
data = str_squish(data)
data = str_replace_all(data, "[^a-zA-Z\\s]","")
}
news$body = clean_function(news$body)
print("Sample of Clean News Article Body:")
cat("\n")
news$body[2]
kable(head(news), format = "latex", booktabs = T,
caption=paste("Table containing Clean News Data")) %>%
kable_styling(latex_options = c("striped","hold_position"))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(reshape)
library(qdap)
library(textclean)
news = news_data%>%
select ( c("id", "pillarName", "sectionName", "webTitle", "body"))%>%
rename(
c(
'pillarName' = 'type',
'sectionName' = 'section',
'webTitle' = 'title'
)
)%>%
na.omit()
abv = c("bn")
rep = c("billion")
clean_function = function(data){
data = str_replace_all(data, "<.*?>","")
data = lapply(data, function(x) gsub("[[:punct:]]"," ",x))
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_curly_quote(data)
data = replace_symbol(data)
data = replace_money(data)
data = replace_abbreviation(data)
data = replace_word_elongation(data)
data = replace_contraction(data)
data = replace_incomplete(data)
data = replace_date(data)
data = replace_ordinal(data)
data = replace_number(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = tolower(data)
data = trimws(data)
data = str_squish(data)
data = str_replace_all(data, "[^a-zA-Z\\s]","")
}
news$body = clean_function(news$body)
print("Columns in the News data before cleaning:")
names(news_data)
kable(head(news_data), format = "latex", booktabs = T,
caption=paste("Table containing Unclean News Data")) %>%
kable_styling(latex_options = c("striped","hold_position"))
library(jsonlite)
library(dplyr)
library(kableExtra)
api = 'https://content.guardianapis.com/search?'
api_key = '0e9a8c4e-1866-4503-a9eb-70e391c499c4'
page_size = '200'
# pages = c(1:5)
# queries = c('business','sports','entertainment','economy','politics','science','crime','health','art','technology')
pages = c(1:2)
queries = c('business','sports')
news_data = data.frame()
data_count = list()
sum_data_count = list()
for(query in queries){
for (page in pages){
url = paste(api, 'q=', query, '&page-size=', page_size, '&page=', page, '&show-fields=body', '&api-key=', api_key,
sep = "")
json = fromJSON(url)
body = as.data.frame(json$response$results$fields)
data = as.data.frame(json$response$results)
data = subset(data, select = -c(fields))
data = cbind(data,body)
data_count = append(data_count,nrow(data))
news_data = rbind(news_data,data)
}
# select_data = select(data, c("sectionName","body"))
# print(kable(head(select_data), format = "latex", booktabs = T,
#         caption=paste("Table containing",toupper(query)," News")) %>%
#     kable_styling(latex_options = c("striped","hold_position")))
# cat("\n")
sum_data_count = append(sum_data_count,sum(as.integer(data_count)))
data_count = 0
}
library(tidyverse)
library(reshape)
library(qdap)
library(textclean)
news = news_data%>%
select ( c("id", "pillarName", "sectionName", "webTitle", "body"))%>%
rename(
c(
'pillarName' = 'type',
'sectionName' = 'section',
'webTitle' = 'title'
)
)%>%
na.omit()
abv = c("bn")
rep = c("billion")
clean_function = function(data){
data = str_replace_all(data, "<.*?>","")
data = lapply(data, function(x) gsub("[[:punct:]]"," ",x))
data = str_replace_all(data, " – ", " ")
data = gsub("\"", " ", data)
data = replace_curly_quote(data)
data = replace_symbol(data)
data = replace_money(data)
data = replace_abbreviation(data)
data = replace_word_elongation(data)
data = replace_contraction(data)
data = replace_incomplete(data)
data = replace_date(data)
data = replace_ordinal(data)
data = replace_number(data)
data = iconv(data,from="UTF-8",to="ASCII//TRANSLIT")
data = tolower(data)
data = trimws(data)
data = str_squish(data)
data = str_replace_all(data, "[^a-zA-Z\\s]","")
}
news$body = clean_function(news$body)
print("Columns in the News data before cleaning:")
names(news_data)
kable(head(news_data), format = "latex", booktabs = T,
caption=paste("Table containing Unclean News Data")) %>%
kable_styling(latex_options = c("striped","hold_position"))
print("Columns in the News data after cleaning:")
names(news)
kable(head(news), format = "latex", booktabs = T,
caption=paste("Table containing Clean News Data")) %>%
kable_styling(latex_options = c("striped","hold_position"))
print("Sample of Unclean News Article Body:")
cat("\n")
news_data$body[2]
print("Sample of Clean News Article Body:")
cat("\n")
news$body[2]
View(news)
install.packages("tm")
install.packages("tm")
install.packages("stopwords")
install.packages("SnowballC")
knitr::opts_chunk$set(echo = TRUE)
library(tm)
install.packages("slam")
library(tm)
install.packages("pacman")
library(tm)
install.packages("tm",dependencies=TRUE)
library(tm)
library(stopwords)
library(SnowballC)
news_body = news$body
tokenized_words = tokenize_word_stems(news_body, stopwords = stopwords::stopwords("en"))
tokenized_words = tokenize_word_stems(news_body, stopwords = stopwords::stopwords("en"))
install.packages("tokenizers")
library(tm)
library(stopwords)
library(tokenizers)
news_body = news$body
tokenized_words = tokenize_word_stems(news_body, stopwords = stopwords::stopwords("en"))
tokenized_words
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
